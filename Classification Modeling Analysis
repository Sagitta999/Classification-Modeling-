#Objective:
#Apprentice has launched a promotion called Halfway There, in this analysis I am about to dig deeper about the insight of this promotion in order to better understand which variables will significantly impact the success or failure of this promotion project. Besides, exploring the business opportunity for optimizing Halfway There will be prioritized in this analysis.

#Step 1:
#Importing the package and file for the model building and business analysis

########################################
# importing packages
########################################
import matplotlib.pyplot as plt                      
import pandas as pd                                  
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LogisticRegression  
from sklearn.metrics import roc_auc_score            
import seaborn           as sns                      

# CART model packages
from sklearn.tree import DecisionTreeClassifier      
from sklearn.tree import export_graphviz             
from sklearn.externals.six import StringIO           
from IPython.display import Image                    
import pydotplus                                     
from sklearn.metrics import confusion_matrix         

# new packages
from sklearn.model_selection import GridSearchCV     
from sklearn.metrics import make_scorer              
import statsmodels.formula.api as smf

from sklearn.neighbors import KNeighborsClassifier   
from sklearn.neighbors import KNeighborsRegressor    
from sklearn.preprocessing import StandardScaler     
########################################
# loading data and setting display options
########################################
# loading data
original_df = pd.read_excel('Apprentice_Chef_Dataset.xlsx')



# setting pandas print options
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
pd.set_option('display.max_colwidth', 100)

#Step 2:
#Feature Engineering for filling missing value, defining email type and setting thresholds

for col in original_df:

   
    if original_df[col].isnull().astype(int).sum() > 0:
        original_df['M_'+col] = original_df[col].isnull().astype(int)

original_df['FAMILY_NAME_SCORE'] = original_df['FAMILY_NAME']
original_df['FAMILY_NAME_SCORE'] = pd.notnull(original_df['FAMILY_NAME_SCORE'])
original_df['FAMILY_NAME_SCORE'] = original_df['FAMILY_NAME_SCORE'].astype(int)
original_df['REVENUE_PER_MEAL'] = original_df['REVENUE']/original_df['TOTAL_MEALS_ORDERED'].round(2)
original_df.head (n=5)

placeholder_lst = []

for index, col in original_df.iterrows():
    split_email = original_df.loc[index, 'EMAIL'].split(sep = '@')
    placeholder_lst.append(split_email)

email_df = pd.DataFrame(placeholder_lst)
################################################
email_df.columns = ['NAME' , 'EMAIL_DOMAIN']
original_df = pd.concat([original_df, email_df['EMAIL_DOMAIN']],
                   axis = 1)
################################################
# email domain types
personal_email_domains = ['@gmail.com','@yahoo.com','@protonmail.com']
work_email_domains  = ['@mmm.com','@amex.com','@merck.com','@jnj.com',
                       '@cocacola.com','@mcdonalds.com','@nike.com',
                       '@apple.com','@ibm.com','@dupont.com','@ge.org',
                       '@microsoft.com','@intel.com','@homedepot.com',
                       '@goldmansacs.com','@unitedtech.com','@cisco.com',
                       '@jpmorgan.com','@pfizer.com','@visa.com',
                       '@disney.com','@walmart.com','@verizon.com',
                       '@caterpillar.com','@pg.com','@chevron.com',
                       '@exxon.com','@boeing.com','@unitedhealth.com',
                       '@travelers.com','@microsoft.com']
junk_email_domains = ['@msn.com','@hotmail.com','@live.com',
                      '@aol.com','@me.com','@passport.com']
# placeholder list
placeholder_lst = []


# looping to group observations by domain type
for domain in original_df['EMAIL_DOMAIN']:
        if '@'+ domain in personal_email_domains:
            placeholder_lst.append('personal') 
        elif '@'+domain in work_email_domains: 
            placeholder_lst.append('pro')
        elif '@'+domain in junk_email_domains: 
            placeholder_lst.append('junk')
        else:
            print('Unknown')

original_df['domain_group'] = pd.Series(placeholder_lst)
original_df['domain_group'].value_counts()

REVENUE_hi = 5500
TOTAL_MEALS_ORDERED_hi = 170
UNIQUE_MEALS_PURCH_hi = 5.5
CONTACTS_W_CUSTOMER_SERVICE_hi = 10
PRODUCT_CATEGORIES_VIEWED_hi = 4
AVG_TIME_PER_SITE_VISIT_hi = 180
CANCELLATIONS_BEFORE_NOON_hi = 6
PC_LOGINS_lo = 5
PC_LOGINS_hi = 6
MOBILE_LOGINS_lo = 0.5
MOBILE_LOGINS_hi = 2.5
WEEKLY_PLAN_hi = 20
EARLY_DELIVERIES_hi = 5
LATE_DELIVERIES_hi = 11
FOLLOWED_RECOMMENDATIONS_PCT_hi = 52
AVG_PREP_VID_TIME_hi = 260
LARGEST_ORDER_SIZE_lo = 2
LARGEST_ORDER_SIZE_hi = 8
MASTER_CLASSES_ATTENDED_hi = 1
MEDIAN_MEAL_RATING_lo = 2
MEDIAN_MEAL_RATING_hi = 4
AVG_CLICKS_PER_VISIT_lo = 8
TOTAL_PHOTOS_VIEWED_hi = 200

##############################################################################
## Feature Engineering (outlier thresholds)                                 ##
##############################################################################
# developing features (columns) for outliers

# REVENUE
original_df['out_REVENUE'] = 0
condition_hi = original_df.loc[0:,'out_REVENUE'][original_df['REVENUE'] > REVENUE_hi]

original_df['out_REVENUE'].replace(to_replace = condition_hi,
                                value      = 1,
                                inplace    = True)
# TOTAL_MEALS_ORDERED
original_df['out_TOTAL_MEALS_ORDERED'] = 0
condition_hi = original_df.loc[0:,'out_TOTAL_MEALS_ORDERED'][original_df['TOTAL_MEALS_ORDERED'] > TOTAL_MEALS_ORDERED_hi]

original_df['out_TOTAL_MEALS_ORDERED'].replace(to_replace = condition_hi,
                                value      = 1,
                                inplace    = True)

# UNIQUE_MEALS_PURCH
original_df['out_UNIQUE_MEALS_PURCH'] = 0
condition_hi = original_df.loc[0:,'out_TOTAL_MEALS_ORDERED'][original_df['UNIQUE_MEALS_PURCH'] > UNIQUE_MEALS_PURCH_hi]

original_df['out_UNIQUE_MEALS_PURCH'].replace(to_replace = condition_hi,
                                value      = 1,
                                inplace    = True)

# CONTACT_W_CUSTOMR_SERVICE
original_df['out_CONTACTS_W_CUSTOMER_SERVICE'] = 0
condition_hi = original_df.loc[0:,'out_CONTACTS_W_CUSTOMER_SERVICE'][original_df['CONTACTS_W_CUSTOMER_SERVICE'] > CONTACTS_W_CUSTOMER_SERVICE_hi]

original_df['out_CONTACTS_W_CUSTOMER_SERVICE'].replace(to_replace = condition_hi,
                                value      = 1,
                                inplace    = True)

# PRODUCT_CATEGORIES_VIEWED
original_df['out_PRODUCT_CATEGORIES_VIEWED'] = 0
condition_hi = original_df.loc[0:,'out_PRODUCT_CATEGORIES_VIEWED'][original_df['PRODUCT_CATEGORIES_VIEWED'] > PRODUCT_CATEGORIES_VIEWED_hi]

original_df['out_PRODUCT_CATEGORIES_VIEWED'].replace(to_replace = condition_hi,
                                value      = 1,
                                inplace    = True)

# AVG_TIME_PER_SITE_VISIT
original_df['out_AVG_TIME_PER_SITE_VISIT'] = 0
condition_hi = original_df.loc[0:,'out_AVG_TIME_PER_SITE_VISIT'][original_df['AVG_TIME_PER_SITE_VISIT'] > AVG_TIME_PER_SITE_VISIT_hi]

original_df['out_AVG_TIME_PER_SITE_VISIT'].replace(to_replace = condition_hi,
                                value      = 1,
                                inplace    = True)

# CANCELLATIONS_BEFORE_NOON
original_df['out_CANCELLATIONS_BEFORE_NOON'] = 0
condition_hi = original_df.loc[0:,'out_CANCELLATIONS_BEFORE_NOON'][original_df['CANCELLATIONS_BEFORE_NOON'] > CANCELLATIONS_BEFORE_NOON_hi]

original_df['out_CANCELLATIONS_BEFORE_NOON'].replace(to_replace = condition_hi,
                                value      = 1,
                                inplace    = True)


# WEEKLY_PLAN
original_df['out_WEEKLY_PLAN'] = 0
condition_hi = original_df.loc[0:,'out_WEEKLY_PLAN'][original_df['WEEKLY_PLAN'] > WEEKLY_PLAN_hi]

original_df['out_WEEKLY_PLAN'].replace(to_replace = condition_hi,
                                value      = 1,
                                inplace    = True)
# EARLY_DELIVERIES
original_df['out_EARLY_DELIVERIES'] = 0
condition_hi = original_df.loc[0:,'out_EARLY_DELIVERIES'][original_df['EARLY_DELIVERIES'] > EARLY_DELIVERIES_hi]

original_df['out_EARLY_DELIVERIES'].replace(to_replace = condition_hi,
                                value      = 1,
                                inplace    = True)
###

# LATE_DELIVERIES
original_df['out_LATE_DELIVERIES'] = 0
condition_hi = original_df.loc[0:,'out_LATE_DELIVERIES'][original_df['LATE_DELIVERIES'] > LATE_DELIVERIES_hi]

original_df['out_LATE_DELIVERIES'].replace(to_replace = condition_hi,
                                value      = 1,
                                inplace    = True)

# FOLLOWED_RECOMMENDATIONS_PCT
original_df['out_FOLLOWED_RECOMMENDATIONS_PCT'] = 0
condition_hi = original_df.loc[0:,'out_FOLLOWED_RECOMMENDATIONS_PCT'][original_df['FOLLOWED_RECOMMENDATIONS_PCT'] > FOLLOWED_RECOMMENDATIONS_PCT_hi]

original_df['out_FOLLOWED_RECOMMENDATIONS_PCT'].replace(to_replace = condition_hi,
                                value      = 1,
                                inplace    = True)

# AVG_PREP_VID_TIME
original_df['out_AVG_PREP_VID_TIME'] = 0
condition_hi = original_df.loc[0:,'out_AVG_PREP_VID_TIME'][original_df['AVG_PREP_VID_TIME'] > AVG_PREP_VID_TIME_hi]

original_df['out_AVG_PREP_VID_TIME'].replace(to_replace = condition_hi,
                                value      = 1,
                                inplace    = True)

# AVG_CLICKS_PER_VISIT
original_df['out_AVG_CLICKS_PER_VISIT'] = 0
condition_lo = original_df.loc[0:,'out_AVG_CLICKS_PER_VISIT'][original_df['AVG_CLICKS_PER_VISIT'] < AVG_CLICKS_PER_VISIT_lo]

original_df['out_AVG_CLICKS_PER_VISIT'].replace(to_replace = condition_lo,
                                    value      = 1,
                                    inplace    = True)

# TOTAL_PHOTOS_VIEWED
original_df['out_TOTAL_PHOTOS_VIEWED'] = 0
condition_lo = original_df.loc[0:,'out_TOTAL_PHOTOS_VIEWED'][original_df['TOTAL_PHOTOS_VIEWED'] > TOTAL_PHOTOS_VIEWED_hi]

original_df['out_TOTAL_PHOTOS_VIEWED'].replace(to_replace = condition_hi,
                                    value      = 1,
                                    inplace    = True)
# PC_LOGINS
original_df['out_PC_LOGINS'] = 0
condition_hi = original_df.loc[0:,'out_PC_LOGINS'][original_df['PC_LOGINS'] > PC_LOGINS_hi]
condition_lo = original_df.loc[0:,'out_PC_LOGINS'][original_df['PC_LOGINS'] < PC_LOGINS_lo]

original_df['out_PC_LOGINS'].replace(to_replace = condition_hi,
                                    value      = 1,
                                    inplace    = True)

original_df['out_PC_LOGINS'].replace(to_replace = condition_lo,
                                    value      = 1,
                                    inplace    = True)

# MOBILE_LOGINS
original_df['out_MOBILE_LOGINS'] = 0
condition_hi = original_df.loc[0:,'out_MOBILE_LOGINS'][original_df['MOBILE_LOGINS'] > MOBILE_LOGINS_hi]
condition_lo = original_df.loc[0:,'out_MOBILE_LOGINS'][original_df['MOBILE_LOGINS'] < MOBILE_LOGINS_lo]

original_df['out_MOBILE_LOGINS'].replace(to_replace = condition_hi,
                                    value      = 1,
                                    inplace    = True)

original_df['out_MOBILE_LOGINS'].replace(to_replace = condition_lo,
                                    value      = 1,
                                    inplace    = True)
# LARGEST_ORDER_SIZE
original_df['out_LARGEST_ORDER_SIZE'] = 0
condition_hi = original_df.loc[0:,'out_LARGEST_ORDER_SIZE'][original_df['LARGEST_ORDER_SIZE'] > LARGEST_ORDER_SIZE_hi]
condition_lo = original_df.loc[0:,'out_LARGEST_ORDER_SIZE'][original_df['LARGEST_ORDER_SIZE'] < LARGEST_ORDER_SIZE_lo]

original_df['out_LARGEST_ORDER_SIZE'].replace(to_replace = condition_hi,
                                    value      = 1,
                                    inplace    = True)

original_df['out_LARGEST_ORDER_SIZE'].replace(to_replace = condition_lo,
                                    value      = 1,
                                    inplace    = True)
# MASTER_CLASSES_ATTENDED
original_df['out_MASTER_CLASSES_ATTENDED'] = 0
condition_hi = original_df.loc[0:,'out_MASTER_CLASSES_ATTENDED'][original_df['MASTER_CLASSES_ATTENDED'] > MASTER_CLASSES_ATTENDED_hi]

original_df['out_MASTER_CLASSES_ATTENDED'].replace(to_replace = condition_hi,
                                value      = 1,
                                inplace    = True)

# MEDIAN_MEAL_RATING
original_df['out_MEDIAN_MEAL_RATING'] = 0
condition_hi = original_df.loc[0:,'out_MEDIAN_MEAL_RATING'][original_df['MEDIAN_MEAL_RATING'] > MEDIAN_MEAL_RATING_hi]
condition_lo = original_df.loc[0:,'out_MEDIAN_MEAL_RATING'][original_df['MEDIAN_MEAL_RATING'] < MEDIAN_MEAL_RATING_lo]

original_df['out_MEDIAN_MEAL_RATING'].replace(to_replace = condition_hi,
                                    value      = 1,
                                    inplace    = True)

original_df['out_MEDIAN_MEAL_RATING'].replace(to_replace = condition_lo,
                                    value      = 1,
                                    inplace    = True)
TOTAL_MEALS_ORDERED_change_hi               = 50000 
UNIQUE_MEALS_PURCH_change_hi                = 10 
CONTACTS_W_CUSTOMER_SERVICE_change_hi       = 10
AVG_TIME_PER_SITE_VISIT_change_hi           = 300
AVG_PREP_VID_TIME_change_hi                 = 300

WEEKLY_PLAN_change_at                       = 30
EARLY_DELIVERIES_change_at                  = 4
LATE_DELIVERIES_change_at                   = 6
MEDIAN_MEAL_RATING_change_at                = 4
MASTER_CLASSES_ATTENDED_change_at           = 2
PRODUCT_CATEGORIES_VIEWED_change_at         = 5
TOTAL_PHOTOS_VIEWED_change_at               = 0

# TOTAL_MEALS_ORDERED
original_df['change_TOTAL_MEALS_ORDERED'] = 0
condition = original_df.loc[0:,'change_TOTAL_MEALS_ORDERED'][original_df['TOTAL_MEALS_ORDERED'] > TOTAL_MEALS_ORDERED_change_hi]

original_df['change_TOTAL_MEALS_ORDERED'].replace(to_replace = condition,
                                   value      = 1,
                                   inplace    = True)



# UNIQUE_MEALS_PURCH
original_df['change_UNIQUE_MEALS_PURCH'] = 0
condition = original_df.loc[0:,'change_UNIQUE_MEALS_PURCH'][original_df['UNIQUE_MEALS_PURCH'] > UNIQUE_MEALS_PURCH_change_hi]

original_df['change_UNIQUE_MEALS_PURCH'].replace(to_replace = condition,
                                value      = 1,
                                inplace    = True)


# CONTACTS_W_CUSTOMER_SERVICE
original_df['change_CONTACTS_W_CUSTOMER_SERVICE'] = 0

condition = original_df.loc[0:,'change_CONTACTS_W_CUSTOMER_SERVICE'][original_df['CONTACTS_W_CUSTOMER_SERVICE'] > CONTACTS_W_CUSTOMER_SERVICE_change_hi]

original_df['change_CONTACTS_W_CUSTOMER_SERVICE'].replace(to_replace = condition,
                                      value      = 1,
                                      inplace    = True)
# AVG_TIME_PER_SITE_VISIT
original_df['change_AVG_TIME_PER_SITE_VISIT'] = 0
condition = original_df.loc[0:,'change_AVG_TIME_PER_SITE_VISIT'][original_df['AVG_TIME_PER_SITE_VISIT'] > AVG_TIME_PER_SITE_VISIT_change_hi]

original_df['change_AVG_TIME_PER_SITE_VISIT'].replace(to_replace = condition,
                                   value      = 1,
                                   inplace    = True)

# AVG_PREP_VID_TIME
original_df['change_AVG_PREP_VID_TIME'] = 0
condition = original_df.loc[0:,'change_AVG_PREP_VID_TIME'][original_df['AVG_PREP_VID_TIME'] > AVG_PREP_VID_TIME_change_hi]

original_df['change_AVG_PREP_VID_TIME'].replace(to_replace = condition,
                                   value      = 1,
                                   inplace    = True)

# WEEKLY_PLAN
original_df['change_WEEKLY_PLAN'] = 0
condition = original_df.loc[0:,'change_WEEKLY_PLAN'][original_df['WEEKLY_PLAN'] == WEEKLY_PLAN_change_at]

original_df['change_WEEKLY_PLAN'].replace(to_replace = condition,
                                       value      = 1,
                                       inplace    = True)
# EARLY_DELIVERIES
original_df['change_EARLY_DELIVERIES'] = 0
condition = original_df.loc[0:,'change_EARLY_DELIVERIES'][original_df['EARLY_DELIVERIES'] == EARLY_DELIVERIES_change_at]

original_df['change_EARLY_DELIVERIES'].replace(to_replace = condition,
                                       value      = 1,
                                       inplace    = True)
# LATE_DELIVERIES
original_df['change_LATE_DELIVERIES'] = 0
condition = original_df.loc[0:,'change_LATE_DELIVERIES'][original_df['LATE_DELIVERIES'] == LATE_DELIVERIES_change_at]

original_df['change_LATE_DELIVERIES'].replace(to_replace = condition,
                                       value      = 1,
                                       inplace    = True)
# MEDIAN_MEAL_RATING
original_df['change_MEDIAN_MEAL_RATING'] = 0
condition = original_df.loc[0:,'change_MEDIAN_MEAL_RATING'][original_df['MEDIAN_MEAL_RATING'] == MEDIAN_MEAL_RATING_change_at]

original_df['change_MEDIAN_MEAL_RATING'].replace(to_replace = condition,
                                       value      = 1,
                                       inplace    = True)
# MASTER_CLASSES_ATTENDED
original_df['change_MASTER_CLASSES_ATTENDED'] = 0
condition = original_df.loc[0:,'change_MASTER_CLASSES_ATTENDED'][original_df['MASTER_CLASSES_ATTENDED'] == MASTER_CLASSES_ATTENDED_change_at]

original_df['change_MASTER_CLASSES_ATTENDED'].replace(to_replace = condition,
                                       value      = 1,
                                       inplace    = True)


# PRODUCT_CATEGORIES_VIEWED
original_df['change_PRODUCT_CATEGORIES_VIEWED'] = 0
condition = original_df.loc[0:,'change_PRODUCT_CATEGORIES_VIEWED'][original_df['PRODUCT_CATEGORIES_VIEWED'] == PRODUCT_CATEGORIES_VIEWED_change_at]

original_df['change_PRODUCT_CATEGORIES_VIEWED'].replace(to_replace = condition,
                                       value      = 1,
                                       inplace    = True)
# TOTAL_PHOTOS_VIEWED
original_df['change_TOTAL_PHOTOS_VIEWED'] = 0
condition = original_df.loc[0:,'change_TOTAL_PHOTOS_VIEWED'][original_df['TOTAL_PHOTOS_VIEWED'] == TOTAL_PHOTOS_VIEWED_change_at]

original_df['change_TOTAL_PHOTOS_VIEWED'].replace(to_replace = condition,
                                       value      = 1,
                                       inplace    = True)
one_hot_domain_group          = pd.get_dummies(original_df['domain_group'])


original_df = original_df.drop('domain_group', axis = 1)

original_df = original_df.join([one_hot_domain_group])


new_columns = original_df.columns

#Step 3:
#Setting train-test split and building a logistic regression to find out the significant variables (P-value < 0.05), the purpose is to find the most-correlate variable and put those into models that I am going to build.

# declaring explanatory variables
original_df_data = original_df.loc[:, ['REVENUE','TOTAL_MEALS_ORDERED','UNIQUE_MEALS_PURCH','CONTACTS_W_CUSTOMER_SERVICE',
                                          'PRODUCT_CATEGORIES_VIEWED','AVG_TIME_PER_SITE_VISIT','MOBILE_NUMBER','CANCELLATIONS_BEFORE_NOON',
                                          'CANCELLATIONS_AFTER_NOON','TASTES_AND_PREFERENCES','PC_LOGINS','MOBILE_LOGINS','WEEKLY_PLAN',
                                          'EARLY_DELIVERIES','LATE_DELIVERIES','PACKAGE_LOCKER','REFRIGERATED_LOCKER','FOLLOWED_RECOMMENDATIONS_PCT',
                                          'AVG_PREP_VID_TIME','LARGEST_ORDER_SIZE','MASTER_CLASSES_ATTENDED','MEDIAN_MEAL_RATING',
                                          'AVG_CLICKS_PER_VISIT', 'TOTAL_PHOTOS_VIEWED','M_FAMILY_NAME','REVENUE_PER_MEAL','junk','personal','pro']]


# declaring response variable
original_df_target = original_df.loc[ : , 'CROSS_SELL_SUCCESS']

# train-test split with stratification
X_train, X_test, y_train, y_test = train_test_split(
            original_df_data,
            original_df_target,
            test_size = 0.25,
            random_state = 508,
            stratify = original_df_target)


# merging training data for statsmodels
original_df_train = pd.concat([X_train, y_train], axis = 1)

# instantiating a logistic regression model object
logistic_full = smf.logit(formula = """ CROSS_SELL_SUCCESS ~
                                        REVENUE +
                                        TOTAL_MEALS_ORDERED +
                                        UNIQUE_MEALS_PURCH +
                                        CONTACTS_W_CUSTOMER_SERVICE +
                                        PRODUCT_CATEGORIES_VIEWED +
                                        AVG_TIME_PER_SITE_VISIT +
                                        MOBILE_NUMBER +
                                        CANCELLATIONS_BEFORE_NOON +
                                        CANCELLATIONS_AFTER_NOON +
                                        TASTES_AND_PREFERENCES +
                                        PC_LOGINS +
                                        MOBILE_LOGINS +
                                        WEEKLY_PLAN +
                                        EARLY_DELIVERIES +
                                        LATE_DELIVERIES +
                                        PACKAGE_LOCKER +
                                        REFRIGERATED_LOCKER +
                                        FOLLOWED_RECOMMENDATIONS_PCT +
                                        AVG_PREP_VID_TIME +
                                        LARGEST_ORDER_SIZE +
                                        MASTER_CLASSES_ATTENDED +
                                        MEDIAN_MEAL_RATING +
                                        AVG_CLICKS_PER_VISIT +
                                        TOTAL_PHOTOS_VIEWED +
                                        M_FAMILY_NAME +
                                        REVENUE_PER_MEAL +
                                        personal +
                                        pro   """,
                                        data    = original_df_train)


# fitting the model object
results_full = logistic_full.fit()


# checking the results SUMMARY
results_full.summary()

# explanatory sets from last session

# creating a dictionary to store candidate models

candidate_dict = {

 # full model
 'logit_full'   : ['REVENUE','TOTAL_MEALS_ORDERED','UNIQUE_MEALS_PURCH','CONTACTS_W_CUSTOMER_SERVICE',
                   'PRODUCT_CATEGORIES_VIEWED','AVG_TIME_PER_SITE_VISIT','MOBILE_NUMBER','CANCELLATIONS_BEFORE_NOON',
                   'CANCELLATIONS_AFTER_NOON','TASTES_AND_PREFERENCES','PC_LOGINS','MOBILE_LOGINS','WEEKLY_PLAN',
                   'EARLY_DELIVERIES','LATE_DELIVERIES','PACKAGE_LOCKER','REFRIGERATED_LOCKER','FOLLOWED_RECOMMENDATIONS_PCT',
                   'AVG_PREP_VID_TIME','LARGEST_ORDER_SIZE','MASTER_CLASSES_ATTENDED','MEDIAN_MEAL_RATING',
                   'AVG_CLICKS_PER_VISIT', 'TOTAL_PHOTOS_VIEWED','M_FAMILY_NAME','REVENUE_PER_MEAL','junk','personal','pro'],

 # significant variables only
 'logit_sig'    : ['MOBILE_NUMBER' , 'CANCELLATIONS_BEFORE_NOON', 'CANCELLATIONS_AFTER_NOON', 'TASTES_AND_PREFERENCES',
                   'FOLLOWED_RECOMMENDATIONS_PCT', 'personal','pro']

}

# instantiating a logistic regression model object
logistic_sig = smf.logit(formula = """ CROSS_SELL_SUCCESS ~
                                        MOBILE_NUMBER +
                                        CANCELLATIONS_BEFORE_NOON +
                                        CANCELLATIONS_AFTER_NOON +
                                        TASTES_AND_PREFERENCES +
                                        FOLLOWED_RECOMMENDATIONS_PCT +
                                        personal +
                                        pro  """,
                                        data    = original_df_train)


# fitting the model object
results_sig = logistic_sig.fit()


# checking the results SUMMARY
results_sig.summary()

#Step 4:
#Defining the function for models.

########################################
# optimal_neighbors
########################################
def optimal_neighbors(X_data,
                      y_data,
                      standardize = True,
                      pct_test=0.25,
                      seed=802,
                      response_type='reg',
                      max_neighbors=20,
                      show_viz=True):
    """
Exhaustively compute training and testing results for KNN across
[1, max_neighbors]. Outputs the maximum test score and (by default) a
visualization of the results.
PARAMETERS
----------
X_data        : explanatory variable data
y_data        : response variable
standardize   : whether or not to standardize the X data, default True
pct_test      : test size for training and validation from (0,1), default 0.25
seed          : random seed to be used in algorithm, default 802
response_type : type of neighbors algorithm to use, default 'reg'
    Use 'reg' for regression (KNeighborsRegressor)
    Use 'class' for classification (KNeighborsClassifier)
max_neighbors : maximum number of neighbors in exhaustive search, default 20
show_viz      : display or surpress k-neigbors visualization, default True
"""    
    
    
    if standardize == True:
        # optionally standardizing X_data
        scaler             = StandardScaler()
        scaler.fit(X_data)
        X_scaled           = scaler.transform(X_data)
        X_scaled_df        = pd.DataFrame(X_scaled)
        X_data             = X_scaled_df



    # train-test split
    X_train, X_test, y_train, y_test = train_test_split(X_data,
                                                        y_data,
                                                        test_size = pct_test,
                                                        random_state = seed)


    # creating lists for training set accuracy and test set accuracy
    training_accuracy = []
    test_accuracy = []
    
    
    # setting neighbor range
    neighbors_settings = range(1, max_neighbors + 1)


    for n_neighbors in neighbors_settings:
        # building the model based on response variable type
        if response_type == 'reg':
            clf = KNeighborsRegressor(n_neighbors = n_neighbors)
            clf.fit(X_train, y_train)
            
        elif response_type == 'class':
            clf = KNeighborsClassifier(n_neighbors = n_neighbors)
            clf.fit(X_train, y_train)            
            
        else:
            print("Error: response_type must be 'reg' or 'class'")
        
        
        # recording the training set accuracy
        training_accuracy.append(clf.score(X_train, y_train))
    
        # recording the generalization accuracy
        test_accuracy.append(clf.score(X_test, y_test))


    # optionally displaying visualization
    if show_viz == True:
        # plotting the visualization
        fig, ax = plt.subplots(figsize=(12,8))
        plt.plot(neighbors_settings, training_accuracy, label = "training accuracy")
        plt.plot(neighbors_settings, test_accuracy, label = "test accuracy")
        plt.ylabel("Accuracy")
        plt.xlabel("n_neighbors")
        plt.legend()
        plt.show()
    
    
    # returning optimal number of neighbors
    print(f"The optimal number of neighbors is: {test_accuracy.index(max(test_accuracy))+1}")
    return test_accuracy.index(max(test_accuracy))+1


########################################
# visual_cm
########################################
def visual_cm(true_y, pred_y, labels = None):
    """
Creates a visualization of a confusion matrix.

PARAMETERS
----------
true_y : true values for the response variable
pred_y : predicted values for the response variable
labels : , default None
    """
    # visualizing the confusion matrix

    # setting labels
    lbls = labels
    

    # declaring a confusion matrix object
    cm = confusion_matrix(y_true = true_y,
                          y_pred = pred_y)


    # heatmap
    sns.heatmap(cm,
                annot       = True,
                xticklabels = lbls,
                yticklabels = lbls,
                cmap        = 'Blues',
                fmt         = 'g')


    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix of the Classifier')
    plt.show()
    
########################################
# display_tree
########################################
def display_tree(tree, feature_df, height = 500, width = 800):
    """
    PARAMETERS
    ----------
    tree       : fitted tree model object
        fitted CART model to visualized
    feature_df : DataFrame
        DataFrame of explanatory features (used to generate labels)
    height     : int, default 500
        height in pixels to which to constrain image in html
    width      : int, default 800
        width in pixels to which to constrain image in html
    """

    # visualizing the tree
    dot_data = StringIO()

    
    # exporting tree to graphviz
    export_graphviz(decision_tree      = tree,
                    out_file           = dot_data,
                    filled             = True,
                    rounded            = True,
                    special_characters = True,
                    feature_names      = feature_df.columns)


    # declaring a graph object
    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())


    # creating image
    img = Image(graph.create_png(),
                height = height,
                width  = width)
    
    return img

########################################
# plot_feature_importances
########################################
def plot_feature_importances(model, train, export = False):
    """
    Plots the importance of features from a CART model.
    
    PARAMETERS
    ----------
    model  : CART model
    train  : explanatory variable training data
    export : whether or not to export as a .png image, default False
    """
    
    # declaring the number
    n_features = X_train.shape[1]
    
    # setting plot window
    fig, ax = plt.subplots(figsize=(12,9))
    
    plt.barh(range(n_features), model.feature_importances_, align='center')
    plt.yticks(pd.np.arange(n_features), train.columns)
    plt.xlabel("Feature importance")
    plt.ylabel("Feature")
    
    if export == True:
        plt.savefig('Tree_Leaf_50_Feature_Importance.png')
        
#Step 5:
#Building Model: Logistic Regression, KNN Classification, Full Tree and Pruned Tree Model to get the highest AUC Score.

#Logistic Regression Model:
# train/test split with the full model
orignial_df_data   =  original_df.loc[ : , candidate_dict['logit_sig']]
orignial_df_target =  original_df.loc[ : , 'CROSS_SELL_SUCCESS']


# this is the exact code we were using before
X_train, X_test, y_train, y_test = train_test_split(
            orignial_df_data,
            orignial_df_target,
            random_state = 802,
            test_size    = 0.25,
            stratify     = orignial_df_target)

# INSTANTIATING a logistic regression model
logreg = LogisticRegression(solver = 'lbfgs',
                            C = 1,
                            random_state = 802)


# FITTING the training data
logreg_fit = logreg.fit(X_train, y_train)


# PREDICTING based on the testing set
logreg_pred = logreg_fit.predict(X_test)


# SCORING the results
print('Training ACCURACY:', logreg_fit.score(X_train, y_train).round(4))
print('Testing  ACCURACY:', logreg_fit.score(X_test, y_test).round(4))
print('AUC SCORE        :', roc_auc_score(y_true  = y_test,
              y_score = logreg_pred).round(4) )

#KNN Classification Model:
# determining the optimal number of neighbors
opt_neighbors = optimal_neighbors(X_data = orignial_df_data,
                                  y_data = orignial_df_target,
                                  response_type= 'class')
                                  
# INSTANTIATING StandardScaler()
scaler = StandardScaler()


# FITTING the data
scaler.fit(orignial_df_data)


# TRANSFORMING the data
X_scaled     = scaler.transform(orignial_df_data)


# converting to a DataFrame
X_scaled_df  = pd.DataFrame(X_scaled) 


# train-test split with the scaled data
X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(
            X_scaled_df,
            orignial_df_target,
            random_state = 802,
            test_size = 0.25,
            stratify = orignial_df_target)


# INSTANTIATING a KNN classification model with optimal neighbors
knn_opt = KNeighborsClassifier(n_neighbors = opt_neighbors)


# FITTING the training data
knn_fit = knn_opt.fit(X_train_scaled, y_train_scaled)


# PREDICTING based on the testing set
knn_pred = knn_fit.predict(X_test_scaled)


# SCORING the results
print('Training ACCURACY:', knn_fit.score(X_train_scaled, y_train_scaled).round(4))
print('Testing  ACCURACY:', knn_fit.score(X_test_scaled, y_test_scaled).round(4))
print('AUC Score        :', roc_auc_score(y_true  = y_test,
                                          y_score = knn_pred).round(4))

#Tree Model:
# INSTANTIATING a classification tree object
full_tree = DecisionTreeClassifier()


# FITTING the training data
full_tree_fit = full_tree.fit(X_train, y_train)


# PREDICTING on new data
full_tree_pred = full_tree_fit.predict(X_test)


# SCORING the model
print('Training ACCURACY:', full_tree_fit.score(X_train, y_train).round(4))
print('Testing  ACCURACY:', full_tree_fit.score(X_test, y_test).round(4))
print('AUC Score        :', roc_auc_score(y_true  = y_test,
                                          y_score = full_tree_pred).round(4))

# INSTANTIATING a classification tree object
tree_pruned      = DecisionTreeClassifier(max_depth = 4,
                                          min_samples_leaf = 25,
                                          random_state = 802)


# FITTING the training data
tree_pruned_fit  = tree_pruned.fit(X_train, y_train)


# PREDICTING on new data
tree_pred = tree_pruned_fit.predict(X_test)


# SCORING the model
print('Training ACCURACY:', tree_pruned_fit.score(X_train, y_train).round(4))
print('Testing  ACCURACY:', tree_pruned_fit.score(X_test, y_test).round(4))
print('AUC Score        :', roc_auc_score(y_true  = y_test,
                                          y_score = tree_pred).round(4))


# calling display_tree
display_tree(tree       = tree_pruned_fit,
             feature_df = X_train)

# plotting feature importance
plot_feature_importances(tree_pruned_fit,
                         train = X_train,
                         export = False)
                         
#Adding Hyperparameter for tuning Logistic Regression Model and Classification Tree Model:

#Optimizing Logistic Regression Model:
########################################
# GridSearchCV
########################################

# declaring a hyperparameter space(give grid search some values to loop over)
C_space          = pd.np.arange(10, 20.0, 10.0)
warm_start_space = [True, False]


# creating a hyperparameter grid
param_grid = {'C'          : C_space, #inputting c values to loop over
              'warm_start' : warm_start_space}#inputting warm start value to loop over


# INSTANTIATING the model object without hyperparameters
lr_tuned = LogisticRegression(solver = 'lbfgs',
                              max_iter = 1000,#setting max_iter =1000 maybe solve the warning,more iterations to converge
                              random_state = 802)


# GridSearchCV object
lr_tuned_cv = GridSearchCV(estimator  = lr_tuned,# which model to use(ie, estimater )
                           param_grid = param_grid,# where are the values for hyperparameter
                           cv         = 3, #how many tests should GridSearchCV do
                           scoring    = make_scorer(roc_auc_score,
                                                    needs_threshold = False))#objective metric


# FITTING to the FULL DATASET (due to cross-validation)
lr_tuned_cv.fit(orignial_df_data, orignial_df_target)


# PREDICT step is not needed


# printing the optimal parameters and best score
print("Tuned Parameters  :", lr_tuned_cv.best_params_)
print("Tuned CV AUC      :", lr_tuned_cv.best_score_.round(4))

# building a model based on hyperparameter tuning results

# INSTANTIATING a logistic regression model with tuned values
lr_tuned = lr_tuned_cv.best_estimator_


# FIT step is not needed


# PREDICTING based on the testing set
lr_tuned_pred = lr_tuned.predict(X_test)


# SCORING the results
print('Training ACCURACY:', lr_tuned.score(X_train, y_train).round(4))
print('Testing  ACCURACY:', lr_tuned.score(X_test, y_test).round(4))
print('AUC Score        :', roc_auc_score(y_true  = y_test,
                                  y_score = lr_tuned_pred).round(4))


#Optimizing Classification Tree Model:
# declaring a hyperparameter space
criterion_space = ['gini', 'entropy']
splitter_space = ['best', 'random']
depth_space = pd.np.arange(1, 2)
leaf_space  = pd.np.arange(1, 2)


# creating a hyperparameter grid
param_grid = {'criterion'        : criterion_space,
              'splitter'         : splitter_space,
              'max_depth'        : depth_space,
              'min_samples_leaf' : leaf_space}


# INSTANTIATING the model object without hyperparameters
tuned_tree = DecisionTreeClassifier(random_state = 555)


# GridSearchCV object
tuned_tree_cv = GridSearchCV(estimator  = tuned_tree,
                             param_grid = param_grid,
                             cv         = 3,
                             scoring    = make_scorer(roc_auc_score,
                                                      needs_threshold = False))


# FITTING to the FULL DATASET (due to cross-validation)
tuned_tree_cv.fit(orignial_df_data, orignial_df_target)


# PREDICT step is not needed


# printing the optimal parameters and best score
print("Tuned Parameters  :", tuned_tree_cv.best_params_)
print("Tuned Training AUC:", tuned_tree_cv.best_score_.round(4))

# building a model based on hyperparameter tuning results

# INSTANTIATING a logistic regression model with tuned values
tree_tuned = tuned_tree_cv.best_estimator_


# FIT step is not needed


# PREDICTING based on the testing set
tree_tuned_pred = tree_tuned.predict(X_test)


# SCORING the results
print('Training ACCURACY:', tree_tuned.score(X_train, y_train).round(4))
print('Testing  ACCURACY:', tree_tuned.score(X_test, y_test).round(4))
print('AUC Score        :', roc_auc_score(y_true  = y_test,
                                          y_score = tree_tuned_pred).round(4))


#Step 6:
#Comparing results for those models that I have created and define the best-performance one.
# comparing results
logreg_fit_score1 = logreg_fit.score(X_train, y_train).round(4)
logreg_fit_score2 = logreg_fit.score(X_test, y_test).round(4)
logreg_fit_score3 = roc_auc_score(y_true  = y_test,
              y_score = logreg_pred).round(4) 

knn_fit_score1 = knn_fit.score(X_train_scaled, y_train_scaled).round(4)
knn_fit_score2 = knn_fit.score(X_test_scaled, y_test_scaled).round(4)
knn_fit_score3 = roc_auc_score(y_true  = y_test,
                                          y_score = knn_pred).round(4)

full_tree_fit_score1 = full_tree_fit.score(X_train, y_train).round(4)
full_tree_fit_score2 = full_tree_fit.score(X_test, y_test).round(4)
full_tree_fit_score3 = roc_auc_score(y_true  = y_test,
                                          y_score = full_tree_pred).round(4)

tree_pruned_fit_score1 = tree_pruned_fit.score(X_train, y_train).round(4)
tree_pruned_fit_score2 = tree_pruned_fit.score(X_test, y_test).round(4)
tree_pruned_fit_score3 = roc_auc_score(y_true  = y_test,
                                          y_score = tree_pred).round(4)

lr_tuned_score1 = lr_tuned.score(X_train, y_train).round(4)
lr_tuned_score2 = lr_tuned.score(X_test, y_test).round(4)
lr_tuned_score3 = roc_auc_score(y_true  = y_test,
                                  y_score = lr_tuned_pred).round(4)

tree_tuned_score1 = tree_tuned.score(X_train, y_train).round(4)
tree_tuned_score2 = tree_tuned.score(X_test, y_test).round(4)
tree_tuned_score3 = roc_auc_score(y_true  = y_test,
                                          y_score = tree_tuned_pred).round(4)



print(f"""
Model                    Training ACCURACY      Testing  ACCURACY               AUC SCORE
-----                    -----------------      -----------------           -----------------
Logistic Regression           {logreg_fit_score1}                  {logreg_fit_score2}                     {logreg_fit_score3}
KNN Classification            {knn_fit_score1}                  {knn_fit_score2}                     {knn_fit_score3}
Full Tree                     {full_tree_fit_score1}                  {full_tree_fit_score2}                     {full_tree_fit_score3}
Pruned Tree                   {tree_pruned_fit_score1}                  {tree_pruned_fit_score2}                     {tree_pruned_fit_score3}
Tuned LR                      {lr_tuned_score1}                  {lr_tuned_score2}                     {lr_tuned_score3}
Tuned Tree                    {tree_tuned_score1}                  {tree_tuned_score2}                     {tree_tuned_score3}
""")
